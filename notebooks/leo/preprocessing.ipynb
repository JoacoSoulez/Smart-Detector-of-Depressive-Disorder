{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5585a4fd-dc40-4686-a374-a99cd2c7c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in /home/leoasad/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (0.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/home/leoasad/.pyenv/versions/3.8.12/envs/lewagon/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leoasad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/leoasad/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run data.ipynb\n",
    "!pip install tweet-preprocessor\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from preprocessor import clean\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9654dc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preprocessor is'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import preprocessor as p\n",
    "p.clean('Preprocessor is #awesome ðŸ‘ https://github.com/s/preprocessor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b5d2fb-4171-4cb7-b046-440201467e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_tweets_and_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270b3f60-8bb6-494d-bf20-0810819282ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\" Replace contractions in the english language by the complete phrase\"\"\"\n",
    "    # Contraction list\n",
    "    contractions = {\n",
    "      \"ain't\": \"am not\",\n",
    "      \"aren't\": \"are not\",\n",
    "      \"can't\": \"cannot\",\n",
    "      \"can't've\": \"cannot have\",\n",
    "      \"'cause\": \"because\",\n",
    "      \"could've\": \"could have\",\n",
    "      \"couldn't\": \"could not\",\n",
    "      \"couldn't've\": \"could not have\",\n",
    "      \"didn't\": \"did not\",\n",
    "      \"doesn't\": \"does not\",\n",
    "      \"don't\": \"do not\",\n",
    "      \"hadn't\": \"had not\",\n",
    "      \"hadn't've\": \"had not have\",\n",
    "      \"hasn't\": \"has not\",\n",
    "      \"haven't\": \"have not\",\n",
    "      \"he'd\": \"he would\",\n",
    "      \"he'd've\": \"he would have\",\n",
    "      \"he'll\": \"he will\",\n",
    "      \"he'll've\": \"he will have\",\n",
    "      \"he's\": \"he is\",\n",
    "      \"how'd\": \"how did\",\n",
    "      \"how'd'y\": \"how do you\",\n",
    "      \"how'll\": \"how will\",\n",
    "      \"how's\": \"how is\",\n",
    "      \"I'd\": \"I would\",\n",
    "      \"I'd've\": \"I would have\",\n",
    "      \"I'll\": \"I will\",\n",
    "      \"I'll've\": \"I will have\",\n",
    "      \"I'm\": \"I am\",\n",
    "      \"I've\": \"I have\",\n",
    "      \"isn't\": \"is not\",\n",
    "      \"it'd\": \"it had\",\n",
    "      \"it'd've\": \"it would have\",\n",
    "      \"it'll\": \"it will\",\n",
    "      \"it'll've\": \"it will have\",\n",
    "      \"it's\": \"it is\",\n",
    "      \"let's\": \"let us\",\n",
    "      \"ma'am\": \"madam\",\n",
    "      \"mayn't\": \"may not\",\n",
    "      \"might've\": \"might have\",\n",
    "      \"mightn't\": \"might not\",\n",
    "      \"mightn't've\": \"might not have\",\n",
    "      \"must've\": \"must have\",\n",
    "      \"mustn't\": \"must not\",\n",
    "      \"mustn't've\": \"must not have\",\n",
    "      \"needn't\": \"need not\",\n",
    "      \"needn't've\": \"need not have\",\n",
    "      \"o'clock\": \"of the clock\",\n",
    "      \"oughtn't\": \"ought not\",\n",
    "      \"oughtn't've\": \"ought not have\",\n",
    "      \"shan't\": \"shall not\",\n",
    "      \"sha'n't\": \"shall not\",\n",
    "      \"shan't've\": \"shall not have\",\n",
    "      \"she'd\": \"she would\",\n",
    "      \"she'd've\": \"she would have\",\n",
    "      \"she'll\": \"she will\",\n",
    "      \"she'll've\": \"she will have\",\n",
    "      \"she's\": \"she is\",\n",
    "      \"should've\": \"should have\",\n",
    "      \"shouldn't\": \"should not\",\n",
    "      \"shouldn't've\": \"should not have\",\n",
    "      \"so've\": \"so have\",\n",
    "      \"so's\": \"so is\",\n",
    "      \"that'd\": \"that would\",\n",
    "      \"that'd've\": \"that would have\",\n",
    "      \"that's\": \"that is\",\n",
    "      \"there'd\": \"there had\",\n",
    "      \"there'd've\": \"there would have\",\n",
    "      \"there's\": \"there is\",\n",
    "      \"they'd\": \"they would\",\n",
    "      \"they'd've\": \"they would have\",\n",
    "      \"they'll\": \"they will\",\n",
    "      \"they'll've\": \"they will have\",\n",
    "      \"they're\": \"they are\",\n",
    "      \"they've\": \"they have\",\n",
    "      \"to've\": \"to have\",\n",
    "      \"wasn't\": \"was not\",\n",
    "      \"we'd\": \"we had\",\n",
    "      \"we'd've\": \"we would have\",\n",
    "      \"we'll\": \"we will\",\n",
    "      \"we'll've\": \"we will have\",\n",
    "      \"we're\": \"we are\",\n",
    "      \"we've\": \"we have\",\n",
    "      \"weren't\": \"were not\",\n",
    "      \"what'll\": \"what will\",\n",
    "      \"what'll've\": \"what will have\",\n",
    "      \"what're\": \"what are\",\n",
    "      \"what's\": \"what is\",\n",
    "      \"what've\": \"what have\",\n",
    "      \"when's\": \"when is\",\n",
    "      \"when've\": \"when have\",\n",
    "      \"where'd\": \"where did\",\n",
    "      \"where's\": \"where is\",\n",
    "      \"where've\": \"where have\",\n",
    "      \"who'll\": \"who will\",\n",
    "      \"who'll've\": \"who will have\",\n",
    "      \"who's\": \"who is\",\n",
    "      \"who've\": \"who have\",\n",
    "      \"why's\": \"why is\",\n",
    "      \"why've\": \"why have\",\n",
    "      \"will've\": \"will have\",\n",
    "      \"won't\": \"will not\",\n",
    "      \"won't've\": \"will not have\",\n",
    "      \"would've\": \"would have\",\n",
    "      \"wouldn't\": \"would not\",\n",
    "      \"wouldn't've\": \"would not have\",\n",
    "      \"y'all\": \"you all\",\n",
    "      \"y'alls\": \"you alls\",\n",
    "      \"y'all'd\": \"you all would\",\n",
    "      \"y'all'd've\": \"you all would have\",\n",
    "      \"y'all're\": \"you all are\",\n",
    "      \"y'all've\": \"you all have\",\n",
    "      \"you'd\": \"you had\",\n",
    "      \"you'd've\": \"you would have\",\n",
    "      \"you'll\": \"you will\",\n",
    "      \"you'll've\": \"you will have\",\n",
    "      \"you're\": \"you are\",\n",
    "      \"you've\": \"you have\"}\n",
    "    \n",
    "    contractions = dict((k.lower(), v.lower()) for k,v in contractions.items())\n",
    "\n",
    "    c_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "    \n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba8bdf4-08f6-4b74-9877-01a1808d70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    \"\"\" Remove numbers \"\"\"\n",
    "    words_only = ''.join([w for w in text if not w.isdigit()])\n",
    "    return words_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e7ee97-efe2-4029-93e2-7cc077221210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_consecutive_duplicates1(text):\n",
    "    \"\"\"Remove consecutive duplicates from text\"\"\"\n",
    "    \n",
    "    def rcd(word):\n",
    "        new_s = \"\"\n",
    "        prev = \"\"\n",
    "        for c in word:\n",
    "            if len(new_s) == 0:\n",
    "                new_s += c\n",
    "                prev = c\n",
    "            if c == prev:\n",
    "                continue\n",
    "            else:\n",
    "                new_s += c\n",
    "                prev = c\n",
    "        return new_s\n",
    "    \n",
    "    english_words = words.words()\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    word_tokens_edited = [rcd(w) if not w.lower() in english_words else w for w in word_tokens ]\n",
    "    \n",
    "    return ' '.join(word_tokens_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33110052-98e7-4f7b-ae11-4c943334425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_consecutive_duplicates2(text):\n",
    "    \"\"\"Remove consecutive duplicates from text\"\"\"\n",
    "    \n",
    "    new_s = \"\"\n",
    "    prev = \"\"\n",
    "    for c in text:\n",
    "        if len(new_s) == 0:\n",
    "            new_s += c\n",
    "            prev = c\n",
    "        if c == prev:\n",
    "            continue\n",
    "        else:\n",
    "            new_s += c\n",
    "            prev = c\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15b147ad-cd13-41aa-8963-05f485f6efe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_consecutive_duplicates2(\"Holaaaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3d4134-3053-4026-b5b3-b42462e07cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis_and_emoticons(text): \n",
    "    \"\"\"\n",
    "    Find emoticons in the text and replace them by a word according to their sentiment\n",
    "    Remove emojis from text\n",
    "    \"\"\"\n",
    "    emoticons_happy = set([\n",
    "        ':-\\)', ':\\)', ';\\)', ':o\\)', ':\\]', ':\\3', ':c\\)', ':>', '=]', '8\\)', '=\\)', ':}',\n",
    "        ':^\\)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "        '=-3', '=3', ':-\\)\\)', ':\\*', ':^\\*', '>:P', ':-P', ':P', 'X-P', \":'-\\)\", \":'\\)\",\n",
    "        'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:\\)', '>;\\)', '>:-\\)','<3'])\n",
    "    \n",
    "    emoticons_happy_compiled = re.compile('(%s)' % \"|\".join(emoticons_happy))\n",
    "    \n",
    "    emoticons_sad = set([':L', ':-/', '>:/', ':S', '>:\\[',':@',':-\\(', ':\\[', '=L', ':<',':-\\|\\|',\n",
    "                    ':-\\[', ':-<', '=\\\\', '=/', '>:\\(', ':\\(', '>.<', \":'-\\(\", \":'\\(\", ':\\\\', ':-c',\n",
    "                     ':c', ':\\{',';\\('])\n",
    "    \n",
    "    emoticons_sad_compiled = re.compile('(%s)' % \"|\".join(emoticons_sad))\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "             u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "             u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "             u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "             u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "             u\"\\U00002702-\\U000027B0\"\n",
    "             u\"\\U000024C2-\\U0001F251\"\n",
    "             \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    \n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = emoticons_happy_compiled.sub(r'happy', text)\n",
    "    text = emoticons_sad_compiled.sub(r'sad', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88cb712c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy leo'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_emojis_and_emoticons(\":-) leo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d6efaeb-e6df-4b96-aeaf-a8a79b7bf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "422f93e4-41f7-4b11-b37b-f1c6caf4321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_symbols(text):\n",
    "    \"\"\"Remove unwanted symbols from text\"\"\"\n",
    "    bad_symbols = re.compile('[^0-9a-z #+_]')\n",
    "    return bad_symbols.sub(' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "866b5404-6db5-43b6-896f-5c3efaf76603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a32169b4-8f37-4363-abc2-1b34fe3c5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    \"\"\" Remove Stop words from text \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.append('https')\n",
    "    stopwords.append('com')\n",
    "    stopwords.append('http')\n",
    "    stopwords.append('twitter')\n",
    "    stopwords.append('m')\n",
    "    stopwords.append('www')\n",
    "    \n",
    "    stop_words = set(stopwords)\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(text) \n",
    "    \n",
    "    filtered_text = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    text = ' '.join(filtered_text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4c9de4e-8487-48f0-9950-1155b7f0149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_and_lemmatizer(text):\n",
    "    \"\"\" Remove Stop words from text \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.append('https')\n",
    "    stopwords.append('com')\n",
    "    stopwords.append('http')\n",
    "    stopwords.append('twitter')\n",
    "    stopwords.append('m')\n",
    "    stopwords.append('www')\n",
    "    \n",
    "    stop_words = set(stopwords)\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(text) \n",
    "    \n",
    "    without_stopwords = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in without_stopwords]\n",
    "    \n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff7a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_context_symbol(text):\n",
    "    import re\n",
    "    return re.sub('<[^>]+>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1e5df5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"69.14\\t71.99\\tParticipant\\t i'm actually feel  i feel pretty good 'cause i uh\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_context_symbol(\"69.14\\t71.99\\tParticipant\\t<clears throat> i'm actually feel <f> i feel pretty good 'cause i uh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c861856-b820-4019-9056-cffe3c7dba34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(texts_sequence):\n",
    "    \"\"\" Return a preprocessed sequence of texts \"\"\"\n",
    "    return texts_sequence.apply(\n",
    "        to_lower).apply(\n",
    "        expand_contractions).apply(\n",
    "        replace_emojis_and_emoticons).apply(\n",
    "        clean).apply(\n",
    "        remove_context_symbol).apply(\n",
    "        remove_bad_symbols).apply(\n",
    "        remove_punctuation).apply(\n",
    "        remove_numbers).apply(\n",
    "        stop_words_and_lemmatizer).apply(\n",
    "        remove_consecutive_duplicates2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88f06b83-ec76-4b1e-b32e-23e02b8f1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba = pd.Series(\"#Devunity Wallpaper  Check this out !   --&gt;    http://twitpic.com/2y2e2 :-) precisssssssion @joaco\")\n",
    "\n",
    "# clean_text(prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fac65d8-fbe7-44b2-96cd-6ef48d07601a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        u mean wek yay wait se bat time bat chanel lol...\n",
       "1        vintage mayfair pink depresion glas candy jar ...\n",
       "2                               martini girl glad un x leg\n",
       "3        son cd left big mark social life point come me...\n",
       "4        know say depresion anger without enthusiasm he...\n",
       "                               ...                        \n",
       "25363                          get voting dianamusic bitch\n",
       "25364    hey lot stuf going right realy strugling finis...\n",
       "25365    kimbo slice joining ultimate fighter cast memb...\n",
       "25366                                   haha pic fabtastic\n",
       "25367            hate fuckin radio thank god ipod ride die\n",
       "Name: text, Length: 25368, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean_text(df['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
